<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>RSE at Sheffield</title><link>http://rse.shef.ac.uk/</link><description>RSE at Sheffield</description><atom:link rel="self" href="http://rse.shef.ac.uk/rss.xml" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Thu, 16 Feb 2017 15:47:06 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>A new member of the team: Tania Allard</title><link>http://rse.shef.ac.uk/blog/tania_allard/</link><dc:creator>Tania Allard</dc:creator><description>&lt;div&gt;&lt;h3&gt;About my research&lt;/h3&gt;
&lt;p&gt;I recently completed a PhD in &lt;a href="http://www.materials.manchester.ac.uk/"&gt;Materials Science at the University of Manchester&lt;/a&gt; which focused on computational nanomechanics. The primary goal was to develop a robust characterisation technique for very small volumes of biocompatible materials and biological tissues.&lt;/p&gt;
&lt;p&gt;Since such materials exhibit highly complex mechanical responses, the extraction of the values of constitutive parameters from test outputs is not straightforward and often requires inverse analysis. For such purposes I used an iterative &lt;a href="https://en.wikipedia.org/wiki/Finite_element_method"&gt;Finite Element Analysis&lt;/a&gt; approach to extract meaningful constitutive parameters from the experimental data.&lt;/p&gt;
&lt;p&gt;The Finite Element simulations were performed using &lt;a href="https://www.3ds.com/products-services/simulia/products/abaqus/"&gt;ABAQUS&lt;/a&gt; while the optimisation based iterative approach was enforced by a series of codes in MATLAB (MATLAB was chosen as it provides a compatible interface to FE codes via multiple programming languages) and Python. The material constitutive laws were prescribed using either user-developed Fortran subroutines or Abaqus-built-in material models. For the case of hydrated materials an additional Fortran subroutine for surface flow conditions was used.&lt;/p&gt;
&lt;p&gt;The workflow was as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python scripts were used to generate the Abaqus input files with user provided variables (e.g. geometrical and boundary conditions identical to the experimental set up)&lt;/li&gt;
&lt;li&gt;The mean experimental response was fit to an analytical expression for time-dependent creep using MATLAB's &lt;a href="https://uk.mathworks.com/help/optim/ug/lsqnonlin.html?requestedDomain=uk.mathworks.com"&gt;lsqnonlin&lt;/a&gt; algorithm. The parameters obtained from the initial fit were then used as the initial  guesses for the optimisation algorithm, after which the Fortran (UMAT) subroutine and/or the input files were updated.&lt;/li&gt;
&lt;li&gt;The FEA was performed  and upon completion the relevant data was extracted using a Python script.&lt;/li&gt;
&lt;li&gt;The MATLAB code was then used to fit the data obtained from FEA to the experimental observations, the parameters of the constitutive model were adjusted by means of the lsqnonlin algorithm. The quality of  the parameter set was evaluated by the minimization of the root mean square error between the experimental and numerical data.&lt;/li&gt;
&lt;li&gt;The parameters are further iteratively refined until the objective function satisfied a given convergence criterion.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once the constitutive material parameters were obtained different parametric studies were performed using the HPC facilites at the University of Manchester (e.g. to study the effect of sample thickness, water content, and support material on the mechanical response).&lt;/p&gt;
&lt;h3&gt;Research Software Engineering&lt;/h3&gt;
&lt;p&gt;I believe there are various reasons which led me to pursue a career as a RSE. While doing my PhD I realized how important research software is, especially when are dealing with highly complex physical systems and when the use of experimental techniques is not enough/too complex/to expensive/unsuitable for what you are studying. Also, I became very frustrated by the lack of Open Source software in my area, especially when we contacted researchers in other institutes, which demanded us signing a waiver to have access to their software. Whenever I found or were passed codes, scripts, or subroutines which would "help" with my research I spent an incredible amount of time going through badly commented (if commented at all!), badly documented scripts with no version control whatsoever. I then imagined it could not only be me getting this frustrated and wasting valuable time trying to make sense of poor code so I started using version control (and forcing people in my lab to do it) and producing code that could be passed to others (more than likely people in my lab).&lt;/p&gt;
&lt;p&gt;Eventually I realized this was a bit of what RSE's do, and it turned out I enjoyed a bit more the software development side of research than the experimental bits, so pursuing a RSE career was pretty much a natural thing to do (and I suppose I wanted to prevent people from getting frustrated when accessing others' scripts). Also, I started realizing the RSE community in the UK is relatively small (albeit constantly growing) so when I saw the advertisement for the position at Sheffield I asked a couple of people at the University of Manchester if they knew the team, I received good comments (mainly on how enthusiastic &lt;a href="http://www.walkingrandomly.com/"&gt;Mike Croucher&lt;/a&gt; is). I did my own research about the University, the RSE team, the projects in different groups, and it seemed both the University and the RSE would provide not only interesting projects to work on but also valuable insight (and mentoring) from experienced RSE's. Also, after realizing that the team was also quite small I figured it would allow for plenty of opportunities to learn loads of new skills while using my current  expertise.&lt;/p&gt;
&lt;p&gt;Last year I volunteered at the &lt;a href="https://www.software.ac.uk/news/2016-05-09-first-ever-conference-research-software-engineers-call-participation"&gt;national RSE conference&lt;/a&gt;, I thought this would be an excellent opportunity to get to know the community, talk to RSE's from different places/universities about the projects they do and why they pursued a career like this. It definitely opened my eyes to the diversity of projects they work in and how collaborative this environment actually is, and if anything it just made me feel more excited and confident about my career post-PhD.&lt;/p&gt;
&lt;p&gt;So when I had the chance to be in the committee for the RSE2017 conference I decided to get involved. Last year was a great experience for me and I think I might have one or two ideas to make this year's event better (even if only a little).&lt;/p&gt;
&lt;p&gt;So far, my experience as a member of the RSE community has been quite pleasant. We always hear about the computer science and STEM communities being not so diverse, but I can see many groups working hard to be more inclusive and working hard to support junior RSE's, like me. The community is filled with very enthusiastic people, often working in very very interesting stuff. The Sheffield RSE team has been very welcoming and supportive over the few months I have been there, so I can truly say that I am very happy to be part of this team.&lt;/p&gt;
&lt;p&gt;I am not 100% sure what my future career looks like, but I would certainly like to help raise awareness of how important software actually is for research, and how important it is for that software to be developed under good practices and with sufficient resources. Many people are aware now how important open data sources are, and I hope people would see research code in a similar way, that it needs to be open and made available for whoever needs it, or just to demonstrate how reproducible their studies are. So I believe I will be making my part by setting/maintaining software standards within the RSE team and spreading the word. Also, I am massively interested in the so-called big-data/data science areas so I would definitely like to get involved in more projects concerning those areas.&lt;/p&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/tania_allard/</guid><pubDate>Thu, 09 Feb 2017 21:37:12 GMT</pubDate></item><item><title>Will Furnass joins to work on Jupyter and Grid Engine integration</title><link>http://rse.shef.ac.uk/blog/willfurnass_intro/</link><dc:creator>Will Furnass</dc:creator><description>&lt;div&gt;&lt;img alt="Will Furnass" class="align-right" src="http://rse.shef.ac.uk/images/willfurnass.png"&gt;
&lt;p&gt;The Research Software Engineering team at the University of Sheffield has gained a new member!
I joined at the start of January and
will primarily be working on &lt;a class="reference external" href="http://opendreamkit.org/"&gt;OpenDreamKit&lt;/a&gt;
which is a &lt;a class="reference external" href="https://ec.europa.eu/programmes/horizon2020/"&gt;Horizon 2020&lt;/a&gt;
European &lt;a class="reference external" href="https://ec.europa.eu/programmes/horizon2020/en/h2020-section/european-research-infrastructures-including-e-infrastructures"&gt;Research Infrastructure&lt;/a&gt; project
with the aim of furthering the open-source computational mathematics ecosystem.&lt;/p&gt;
&lt;p&gt;My contribution to this project is to extend work previously started at the University of Sheffield
to allow researchers to more easily run interactive workflows on &lt;a class="reference external" href="https://en.wikipedia.org/wiki/High-performance_computing"&gt;High-Performance Computing&lt;/a&gt; clusters,
specifically to make it easy, robust and intuitive to run &lt;a class="reference external" href="http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html"&gt;Jupyter Notebooks&lt;/a&gt; on
clusters running job scheduling software from the &lt;a class="reference external" href="https://arc.liv.ac.uk/trac/SGE"&gt;Grid Engine&lt;/a&gt; family.&lt;/p&gt;
&lt;p&gt;Jupyter Notebooks are &lt;strong&gt;runnable documents&lt;/strong&gt; containing code snippets
that are viewed and manipulated from a web browser.
They are an increasingly popular way of encapsulating, presenting and sharing a coding-oriented workflow.
A Notebook comprises a column of cells, where each cell can contain:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;some code or&lt;/li&gt;
&lt;li&gt;explanatory text (that can be formatted using &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Markdown"&gt;Markdown&lt;/a&gt;) and/or mathematical expressions (formatted using &lt;a class="reference external" href="https://www.mathjax.org/"&gt;MathJax&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a code cell is executed by the user it can return anything renderable by a modern web browser:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;a single value,&lt;/li&gt;
&lt;li&gt;a table of data,&lt;/li&gt;
&lt;li&gt;a figure or&lt;/li&gt;
&lt;li&gt;a mathematical expression.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;img alt="/images/jupyter_notebook_example.png" src="http://rse.shef.ac.uk/images/jupyter_notebook_example.png"&gt;
&lt;p&gt;The code cells of a Notebook can be (re)run in any order, so Notebooks are very useful for interactive exploration.&lt;/p&gt;
&lt;p&gt;The structure of Jupyter is typically as follows:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[Notebook in browser] &amp;lt;---&amp;gt; [Jupyter server] &amp;lt;---&amp;gt; [Kernel]
&lt;/pre&gt;
&lt;p&gt;where the kernel is the part that executes code cells.  There are &lt;a class="reference external" href="https://github.com/jupyter/jupyter/wiki/Jupyter-kernels"&gt;kernels for many different programming languages&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;The server and kernel can run on the same machine as the web browser
but the architecture allows them to also run on remote machines.
These remote systems could be:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;a research group's central server,&lt;/li&gt;
&lt;li&gt;a Jupyter-aware cloud service (e.g. &lt;a class="reference external" href="https://cloud.sagemath.com/settings"&gt;SageMathCloud&lt;/a&gt; or &lt;a class="reference external" href="https://notebooks.azure.com/"&gt;Azure Notebooks&lt;/a&gt;) or ...&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the HPC clusters operated by so many academic institutions&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is where my part of OpenDreamKit comes in.
Computer clusters such as &lt;a class="reference external" href="http://docs.hpc.shef.ac.uk/en/latest/iceberg/index.html"&gt;Iceberg&lt;/a&gt; and &lt;a class="reference external" href="http://docs.hpc.shef.ac.uk/en/latest/sharc/index.html"&gt;ShARC&lt;/a&gt; here at the University of Sheffield allow users to
run computational jobs with more resources than typically available in researchers' own machines.
Jobs can have parallel threads of execution running on up to sixteen cores per node
and/or running over multiple nodes,
jobs can use hundreds of MB of RAM and
can make use of the latest generation of GPUs for things like
accelerated &lt;a class="reference external" href="http://www.acrc.com/deep-learning/"&gt;deep learning workflows&lt;/a&gt;.
However, the need to request resources, then submit and monitor jobs from the command-line can be a steep barrier to entry for some.
Being able to easily run Jupyter Notebooks on our clusters and
request the necessary resources for our interactive explorations via an intuitive web interface
could help make HPC more accessible and useful to those without a strong understanding of Linux and the command-line.&lt;/p&gt;
&lt;p&gt;We already have &lt;a class="reference external" href="http://docs.hpc.shef.ac.uk/en/latest/iceberg/accessing/jupyterhub.html"&gt;an instance of JupyterHub running&lt;/a&gt; to allow users to start Jupyter sessions on our &lt;a class="reference external" href="http://docs.hpc.shef.ac.uk/en/latest/iceberg/index.html"&gt;Iceberg&lt;/a&gt; cluster
thanks to the efforts of &lt;a class="reference external" href="http://stuartmumford.uk/"&gt;Stuart Mumford&lt;/a&gt;.
I will be working on:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Upgrading this to use the latest version of JupyterHub;&lt;/li&gt;
&lt;li&gt;Setting up JupyterHub on our new cluster (&lt;a class="reference external" href="http://docs.hpc.shef.ac.uk/en/latest/sharc/index.html"&gt;ShARC&lt;/a&gt;);&lt;/li&gt;
&lt;li&gt;Developing a mechanism for easily requesting resources (more RAM / CPU cores / GPUs) from the Grid Engine scheduler;&lt;/li&gt;
&lt;li&gt;Making the JupyterHub and Grid Engine integration more robust.&lt;/li&gt;
&lt;li&gt;Looking at how JupyterHub could be set up on HPC clusters at other institutions (possibly using different schedulers) for research/teaching.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm rather excited about this new role.  One nice aspect to it is that I am now according to my contract officially a &lt;a class="reference external" href="http://www.rse.ac.uk/"&gt;Research Software Engineer&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Dear Dr Furnass&lt;/p&gt;
&lt;p&gt;Further to recent discussions, I am pleased to confirm the change in your appointment with the University of Sheffield.
The details of your offer are provided below:&lt;/p&gt;
&lt;p&gt;Appointment Details:
You, Dr William Furnass, shall be employed by the University of Sheffield as a Research Software Engineer
in the Department of Computer Science with effect from 1 January 2017.
This position is offered on a fixed term basis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This demonstrates that research institutions have started recognising &lt;a class="reference external" href="http://www.rse.ac.uk/who.html"&gt;Research Software Engineering&lt;/a&gt; as an alternative career path in academia
(something the &lt;a class="reference external" href="https://www.software.ac.uk/"&gt;Software Sustainability Institute&lt;/a&gt; have been pushing for for some time) and
RSEs aren't simply post-doctoral researchers who happen to write software.&lt;/p&gt;
&lt;p&gt;The path to this point has not been particularly direct:
I have a &lt;a class="reference external" href="https://engineering.leeds.ac.uk/computing"&gt;computer scence degree&lt;/a&gt;,
worked as a IT systems engineer in the film industry,
have a PhD plus post-doc experience in &lt;a class="reference external" href="http://www.shef.ac.uk/civil/"&gt;water engineering&lt;/a&gt;
(where I developed semi-physical and data-driven models of water quality in water distribution networks) and
I have provided support to the users of the University of Sheffield's &lt;a class="reference external" href="http://docs.hpc.shef.ac.uk/"&gt;HPC clusters&lt;/a&gt;.
In addition I taught or helped run RSE, water engineering and study skills workshops.&lt;/p&gt;
&lt;p&gt;My interests include
helping researchers optimise data analysis workflows (primarily using higher-level languages),
providing training in RSE best practices and
systems administration.
You can contact me via:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Email: w.furnass (at) sheffield.ac.uk&lt;/li&gt;
&lt;li&gt;Twitter: &lt;a class="reference external" href="https://twitter.com/willfurnass"&gt;@willfurnass&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/willfurnass_intro/</guid><pubDate>Mon, 30 Jan 2017 10:56:00 GMT</pubDate></item><item><title>£1 million grant to shed light on how we learn languages</title><link>http://rse.shef.ac.uk/blog/linuistics_grant_2016/</link><dc:creator>Mike Croucher</dc:creator><description>&lt;div&gt;&lt;p&gt;A £1 million grant to help researchers understand what speakers know about languages, in order to help make learning foreign languages easier, has been awarded to the University of Sheffield's Faculty of Arts and Humanities.  &lt;/p&gt;
&lt;p&gt;Over five years, the Research Leadership Award from the Leverhulme Trust will allow experts to develop new, accurate ways of describing speakers’ linguistic knowledge, by using machine-learning techniques that mimic the way in which humans learn.&lt;/p&gt;
&lt;p&gt;The patterns they find will be verified in laboratory settings and then tested on adult foreign language learners to see if such patterns can help them learn a foreign language in a way that resembles how they learned their mother tongue.&lt;/p&gt;
&lt;p&gt;The aim is to lead a step-change in research on language and language learning by capturing the linguistic knowledge adult speakers build up when they are exposed to a language in natural settings. These insights will help with the development of strategic language teaching materials to transform the way in which we teach foreign languages.&lt;/p&gt;
&lt;p&gt;The team will be led by Dr Dagmar Divjak from the University’s School of Languages and Cultures, in close collaboration with Dr Petar Milin, Department of Journalism Studies, and with Research Software Engineering support from Dr Mike Croucher, Department of Computer Science.&lt;/p&gt;
&lt;p&gt;Sheffield's Research Software Engineering Group are collaborators on the project and will provide support in High Performance Computing, software engineering and data management. This will help ensure that all developed software is efficient, correct, citable, easy to use and openly available. The aim is to maximise research impact and reproducibility through the application of modern software engineering methodologies.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The out of our minds team" src="http://rse.shef.ac.uk/images/DSC03577.jpg"&gt;&lt;/p&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/linuistics_grant_2016/</guid><pubDate>Tue, 03 Jan 2017 15:22:11 GMT</pubDate></item><item><title>Bashing down Windows for Materials Science</title><link>http://rse.shef.ac.uk/blog/windows_bash_materials/</link><dc:creator>Christopher Handley</dc:creator><description>&lt;div&gt;&lt;p&gt;In the last few months Windows 10 has had an interesting new capability – &lt;a href="http://www.walkingrandomly.com/?p=6187"&gt;Bash&lt;/a&gt;. Originally the Linux Subsystem was only available for those on the developer loop, but since the Windows 10 Anniversary edition this subsystem has been available to all users who activate it. The subsystem is not an emulator, but a way for Windows 10 to run Linux applications, and to use the Linux Bash environment, through the use of dynamic maps between Linux system calls and Windows ones.&lt;/p&gt;
&lt;p&gt;As a computational chemist working in the &lt;a href="http://www.sheffield.ac.uk/materials"&gt;Department of Materials Science and Engineering&lt;/a&gt; this is really an excellent and exciting new way that Windows has evolved.
There are a great many tools for my research. Some work on Windows, and are well designed for that OS, given that they are applications aimed at the people that make and analyse their materials. These tools help users visualize crystal structures in 3D, or predict from crystal structures experimental observables, such as transition electron microscopy. For computational chemists, these tools are often also invaluable as they allow us to construct visually the crystal structures that we wish to then simulate using quantum mechanics or classical force fields.
More often than not, the programs designed for running such chemical simulations, have no GUI, and run in a Unix environment. &lt;a href="http://www.castep.org/"&gt;CASTEP&lt;/a&gt; is a UK created Density Functional Theory simulation package, which is free for all UK academics, and is used extensively by those researchers wishing to simulated solid state materials, such as batteries, piezoelectric materials, and solar power materials. Previously, to run CASTEP on a Windows machine, &lt;a href="https://www.cygwin.com/"&gt;Cygwin&lt;/a&gt; or a &lt;a href="https://en.wikipedia.org/wiki/Virtual_machine"&gt;virtual machine&lt;/a&gt; were required. However, with the new subsystem, CASTEP installs out of the box as if you were running any other Linux computer.
The same is equally true of &lt;a href="https://nanochemistry.curtin.edu.au/gulp/"&gt;GULP&lt;/a&gt;, another program used in materials science, which is often used to design, test, and analyse atomistic potentials. &lt;a href="http://www.scd.stfc.ac.uk/SCD/44516.aspx"&gt;DL_POLY&lt;/a&gt;, another UK created simulation package is also used by a large user base to perform molecular dynamics simulations using atomistic potentials.&lt;/p&gt;
&lt;p&gt;All of the above programs mentioned, and many more, such as the DFT codes &lt;a href="https://www.vasp.at/"&gt;VASP&lt;/a&gt; and &lt;a href="http://www.wien2k.at/"&gt;WIEN2K&lt;/a&gt;, and other molecular dynamics programs such as &lt;a href="http://www.gromacs.org/"&gt;GROMACS&lt;/a&gt;, and &lt;a href="http://lammps.sandia.gov/"&gt;LAMMPS&lt;/a&gt;, can have their output analysed by these Windows 10 packages, and their inputs easily designed by these same crystal analysis programs, but natively are best run in a Unix environment.&lt;/p&gt;
&lt;p&gt;The typical work around has always been either the use a virtual machine, Cygwin, or, using more expensive Apple computers, or making users use Linux machines for which they may not be comfortable using – especially if their previous workflow used packages that ran on Windows.&lt;/p&gt;
&lt;p&gt;Personally I fall into that last category of users. While I can write a paper in &lt;a href="https://www.latex-project.org/"&gt;LaTeX&lt;/a&gt;, I really don’t like it compared to the WYSIWYG world of Word, and of course with word I can use my favourite citation manager, Zotero (which by the way the work around using Dropbox is also good fun). That impact on workflow is an important thing, especially if you are dealing with final year students who you want to work on your research. Ideally you want to get them up and running ASAP where the only teaching you need to do is how to run the simulation packages. I don’t want to have to teach them how to use and entirely new OS, and in the case of Linux, perhaps entirely new ways to write documents and make spreadsheets. This is especially true if the university course from the first year onwards has included access to MS Office, and has done teaching using those tools.&lt;/p&gt;
&lt;p&gt;By being able to now run many of these simulation packages through the Windows Bash Linux subsystem there are minimal hoops to jump through. All students now have easy access to a machine that can run the simulation programs, and without having to switch OS, or log into a dedicated UNIX server which is maintained for PhD and postdoc research. That lack of need to use a virtual machine, or emulator, also means much less impact on resources on personal machines, and less peculiarities with the allocation of computing resources on those machine. Furthermore, with respect to workflows, inputs and outputs from those simulation packages all can happen under the one roof of the Windows 10 OS, and lead to greater productivity.&lt;/p&gt;
&lt;p&gt;Bash in Windows 10 has trampled down a barrier which makes the use of the OS far more competitive, cost effective and productive for computational chemistry.&lt;/p&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/windows_bash_materials/</guid><pubDate>Thu, 03 Nov 2016 16:33:11 GMT</pubDate></item><item><title>A new member of the team: Mozhgan Kabiri Chimeh</title><link>http://rse.shef.ac.uk/blog/mozhgankch_intro/</link><dc:creator>Mozhgan Kabiri Chimeh</dc:creator><description>&lt;div&gt;&lt;p&gt;My name is &lt;strong&gt;Mozhgan Kabiri Chimeh&lt;/strong&gt; and I am a &lt;a href="http://www.rse.ac.uk/"&gt;Research Associate/Research Software Engineer&lt;/a&gt; who specialises in performance acceleration targeting Many-core and Multi-core architectures. Research is my passion and I have carefully developed my education with research and teaching in mind. I completed my PhD in computer science in 2016 at the University of Glasgow where my area of research was accelerating logic gate circuit simulation targeting heterogeneous architectures.  As part of my PhD project, I optimised and accelerated simulation algorithms and applied them to various parallel architectures (SIMD enabled machines, clusters, and GPUs). I have practical experience with parallel programming using High Performance Computing languages and models including OpenMP and CUDA.&lt;/p&gt;
&lt;p&gt;I am glad to be a part of RSE team as well as working as a researcher in Computer Graphic and simulation modelling group here at the University of Sheffield. Feel free to get in touch with me via my email address (m.kabiri-chimeh (at) sheffield.ac.uk) or my &lt;a href="https://uk.linkedin.com/in/mozhgankch"&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When not working I divide my time between family, movie, artwork and macro-photography!&lt;/p&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/mozhgankch_intro/</guid><pubDate>Fri, 28 Oct 2016 08:09:22 GMT</pubDate></item><item><title>Manchester Julia Workshop</title><link>http://rse.shef.ac.uk/blog/julia-manchester-2016/</link><dc:creator>Avgoustinos Vouros</dc:creator><description>&lt;div&gt;&lt;p&gt;A few weeks ago (19-20th September 2016) I had the chance to attend the very first Julia workshop in the UK held at the University of Manchester by the &lt;a href="http://www.maths.manchester.ac.uk/~siam/"&gt;SIAM Student Chapter&lt;/a&gt;. The first day of the &lt;a href="http://www.maths.manchester.ac.uk/~siam/julia16/"&gt;workshop&lt;/a&gt; consisted of a basic tutorial of Julia, installation instructions and around five hours of hackathon. The second day provided an introduction to carrying out research in various fields such as data analysis, material science, natural language processing and bioinformatics using Julia. The attendees were a mixture of PhD students, post-docs and lecturers mainly from the University of Manchester as well as other universities and institutes (Warwick, Glasgow, Reading, MIT, Imperial College London, Earlham Institute).&lt;/p&gt;
&lt;h2&gt;Day 1: Tutorial and Hackathon&lt;/h2&gt;
&lt;p&gt;There are several ways to &lt;a href="http://julialang.org/downloads/"&gt;run Julia&lt;/a&gt; in any OS, including command line version, &lt;a href="http://junolab.org/"&gt;Juno&lt;/a&gt; IDE and Jupyter notebook (&lt;a href="https://github.com/JuliaLang/IJulia.jl"&gt;IJulia&lt;/a&gt;). In case you want to avoid any installation process then there is also the  browser based &lt;a href="https://www.juliabox.com/"&gt;JuliaBox.com&lt;/a&gt;. I was surprised that the whole process was smooth without any software setup issues!&lt;/p&gt;
&lt;p&gt;The tutorial consisted of some very basic demonstration of Julia mostly on linear algebra and statistics and after a short break we were left to explore Julia, collaborate and exchange ideas. There were also two Exercises proposed to us:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/street-view-getting-started-with-julia"&gt;First Steps With Julia&lt;/a&gt; by &lt;a href="https://www.kaggle.com/"&gt;kaggle&lt;/a&gt; which teaches some basics of image processing and machine learning to identify the character from pictures.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/document/d/1NYQFLeORdMCHl2hncOkhNNPNJUWGAoGcR_tCs-KF-D0/edit#heading=h.262rsr2l8kb4"&gt;Bio.jl Exercises&lt;/a&gt; by Ben J. Ward which provides simple examples of using the &lt;a href="https://github.com/BioJulia/Bio.jl"&gt;Bio.jl&lt;/a&gt; to do simple operations and manipulations of biological sequences.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I wanted to try as many libraries as possible from image processing and data visualization to embedded Java, I ended up using a lot of different packages so I found these commands (self-explanatory) for package managing the most useful for me:&lt;code&gt;Pkg.add("PackageName")&lt;/code&gt;, &lt;code&gt;Pkg.status()&lt;/code&gt;, &lt;code&gt;Pkg.update()&lt;/code&gt;. Here of course, I detected some compatibility issues. I was running Julia version 0.4.6 but it appears that most of the attendees were using the version 0.4.5. Some commands seemed to have changed between these versions; for example in the kaggle's exercise the command &lt;code&gt;float32sc(img)&lt;/code&gt; which converts an image to float values was not working for me instead I had to use the &lt;code&gt;float32(img)&lt;/code&gt; command. A minor issue for a new-born language.&lt;/p&gt;
&lt;h2&gt;Day 2: Talks&lt;/h2&gt;
&lt;p&gt;The talks were centred around specific fields with heavy scientific computing (automatic differentiation, molecular modelling, natural language processing, bioinformatics and computational biology) and how Julia influence these fields. Each speaker presented his field of research and his Julia implementations which ended up as another package for the Julia community. More information about the speakers can be found on the &lt;a href="http://www.maths.manchester.ac.uk/~siam/julia16/"&gt;Manchester Julia Workshop webpage&lt;/a&gt; and a list of the presented packages can be found below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data analysis and visualization: &lt;a href="http://distributionsjl.readthedocs.io/en/latest/"&gt;Distributions.jl&lt;/a&gt;, &lt;a href="https://github.com/JuliaStats/DataFrames.jl"&gt;DataFrames.jl&lt;/a&gt;, &lt;a href="https://github.com/JuliaStats/GLM.jl"&gt;GLM.jl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Embed other languages with Julia (Python, R): &lt;a href="https://github.com/JuliaPy/PyCall.jl"&gt;PyCall.jl&lt;/a&gt;, &lt;a href="https://github.com/JuliaStats/RCall.jl"&gt;RCall.jl&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Automatic differentiation: &lt;a href="https://github.com/JuliaDiff/ForwardDiff.jl"&gt;ForwardDiff.jl&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Molecular modelling: &lt;a href="https://github.com/libAtoms/JuLIP.jl"&gt;JuLIP.jl&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Bioinformatics and computational biology: &lt;a href="https://github.com/BioJulia/Bio.jl"&gt;bio.jl&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Final words&lt;/h2&gt;
&lt;p&gt;Overall I was very satisfied with the Julia experience and I am waiting for its first official release (v1.0) which will probably be next year. Here are the main advantages which led me to believe that Julia can be the next on demand programming language for scientific computing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Combines the productivity of dynamic languages (Java, Python) and the performance of static languages (C, Fortran). In other words: very easy to write optimized code and run your program fast at the same time. &lt;a href="https://jiahao.github.io/"&gt;Dr Jiahao Chen&lt;/a&gt; from MIT in his &lt;a href="http://www.slideshare.net/acidflask/programming-languages-history-relativity-and-design"&gt;talk&lt;/a&gt; mentioned the following referring to Julia's speed, "&lt;em&gt;You can define many methods for a generic function. If the compiler can figure out exactly which method you need to use when you invoke a function, then it generates optimized code&lt;/em&gt;".&lt;/li&gt;
&lt;li&gt;Deals with the two language problem: base library and functionality is written in Julia itself.&lt;/li&gt;
&lt;li&gt;It is free and open source (MIT licensed), high advantageous for the scientific community to share code or expand existing one.&lt;/li&gt;
&lt;li&gt;A great and friendly community and users from various fields which constantly expand the existing Julia library.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Fun fact:&lt;/strong&gt; The system for variable declaration accepts any &lt;a href="http://docs.julialang.org/en/release-0.5/manual/unicode-input/"&gt;Unicode character&lt;/a&gt;: &lt;code&gt;\delta[tab] = 2&lt;/code&gt; results in &lt;code&gt;δ = 2&lt;/code&gt;, &lt;code&gt;\:smiley: = 4&lt;/code&gt; results in &lt;code&gt;😃 = 4.&lt;/code&gt; Although, apart from some April Fool's pranks, Julia's &lt;a href="http://docs.julialang.org/en/release-0.5/manual/variables/"&gt;stylistic conventions&lt;/a&gt; is advised to be followed when defining variable names!&lt;/p&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/julia-manchester-2016/</guid><pubDate>Wed, 19 Oct 2016 11:46:11 GMT</pubDate></item><item><title>Coffee and Cakes Event</title><link>http://rse.shef.ac.uk/blog/coffee-and-cakes-event/</link><dc:creator>Paul Richmond</dc:creator><description>&lt;div&gt;&lt;p&gt;RSE Sheffield is hosting its first coffee and cakes event on &lt;strong&gt;4th October 2016 at 10:00 in the Ada Lovelace&lt;/strong&gt; room on 1st floor of the Computer Science Department (Regents Court East). Attendance is free and you don't need to register (or bring coffee and cake with you). Simply call in and take the opportunity to come and have an informal chat about research software.&lt;/p&gt;
&lt;p&gt;The event is a community event for anyone not just computer science or members of the RSE team. If you work on software development are an RSE or simply want to talk about some aspect of software or software in teaching then come along.&lt;/p&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/coffee-and-cakes-event/</guid><pubDate>Mon, 26 Sep 2016 14:42:42 GMT</pubDate></item><item><title>Accelerated versions of R for Iceberg</title><link>http://rse.shef.ac.uk/blog/intel-R-iceberg/</link><dc:creator>Mike Croucher</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;To Long; Didn't Read -- Summary&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I've built a version of R on Iceberg that is faster than the standard version for various operations. Documentation is at &lt;a href="http://docs.hpc.shef.ac.uk/en/latest/iceberg/software/apps/r.html"&gt;http://docs.hpc.shef.ac.uk/en/latest/iceberg/software/apps/r.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If it works more quickly for you, or if you have problems, please let us know by emailing &lt;a href="mailto:rse@sheffield.ac.uk"&gt;rse@sheffield.ac.uk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I took over building &lt;a href="http://docs.hpc.shef.ac.uk/en/latest/iceberg/software/apps/r.html"&gt;R for Iceberg&lt;/a&gt;, Sheffield's High Performance Computing System, around a year ago and have been incrementally improving both the install and the documentation with every release. Something that's been bothering me for a while is the lack of optimisation. The standard Iceberg build uses an ancient version of the gcc compiler and (probably) unoptimised versions of &lt;a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms"&gt;BLAS&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/LAPACK"&gt;LAPCK&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;BLAS and LAPACK are extremely important libraries -- they provide the code that programs such as R use for linear algebra: Matrix-Matrix multiplication, Cholesky decomposition, principle component analysis and so on. It's important to note that there are lots of implementations of BLAS and LAPACK: &lt;a href="http://math-atlas.sourceforge.net/"&gt;ATLAS&lt;/a&gt;, &lt;a href="http://www.openblas.net/"&gt;OpenBLAS&lt;/a&gt; and the &lt;a href="https://software.intel.com/en-us/intel-mkl"&gt;Intel MKL&lt;/a&gt; are three well-known examples. Written in Fortran, the interfaces of all of these versions are identical, which means you can use them interchangeably, but the speed of the implementation can vary considerably.&lt;/p&gt;
&lt;p&gt;The BLAS and LAPACK implementations on Iceberg are undocumented (before my time!) which means that we have no idea what we are dealing with. Perhaps they are optimised, perhaps not. I suspected 'not'.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Building R with the Intel Compiler and MKL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Intel Compiler Suite often produces the fastest executables of all available compilers for any given piece of Fortran or C/C++ code. Additionally, the Intel MKL is probably the fastest implementation of BLAS and LAPACK available for Intel Hardware. As such, I've had &lt;strong&gt;Build R using Intel Compilers and MKL&lt;/strong&gt; on my to-do list for some time.&lt;/p&gt;
&lt;p&gt;Following a recent visit to the University of Lancaster, where they've been doing this for a while, I finally bit the bullet and produced some build-scripts. Thanks to Lancaster's Mike Pacey for help with this!  There are two versions (links point to the exact commits that produced the builds used in this article):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/mikecroucher/HPC_Installers/blob/ea4a9f33b705a8cae01841d9c173278fcb486061/apps/R/3.3.1/sheffield/iceberg/intel_15/install_intel_r_sequential.sh"&gt;install_intel_r_sequential.sh&lt;/a&gt; - Linked to the sequential (i.e. single-core) version of Intel MKL.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mikecroucher/HPC_Installers/blob/ea4a9f33b705a8cae01841d9c173278fcb486061/apps/R/3.3.1/sheffield/iceberg/intel_15/install_intel_r_parallel.sh"&gt;install_intel_r_parallel.sh&lt;/a&gt; - Linked to the parallel version of Intel MKL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The benchmark code is available in the Sheffield HPC examples repo &lt;a href="https://github.com/mikecroucher/HPC_Examples/"&gt;https://github.com/mikecroucher/HPC_Examples/&lt;/a&gt;. The exact commit that produced these results is &lt;a href="https://github.com/mikecroucher/HPC_Examples/blob/35de11e7c47bc278b15a64fb77c5575b074e1a47/languages/R/linear_algebra/linear_algebra_bench.r"&gt;35de11e&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It's no good having fast builds of R if they give the wrong results! To make sure that everything is OK, I ran R's installation test suite and everything passed. If you have an account on iceberg, you can see the output from the test suite at &lt;code&gt;/usr/local/packages6/apps/intel/15/R/sequential-3.3.1/install_logs/make_install_tests-R-3.3.1.log&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It's important to note that although the tests passed, there &lt;strong&gt;are&lt;/strong&gt; differences in output between this build and the reference build that R's test suite is based on. This is due to a number of factors such as the fact that &lt;a href="http://www.walkingrandomly.com/?p=5380"&gt;Floating point addition is not associative&lt;/a&gt; and that the signs of eigenvectors are arbitrary and so on.&lt;/p&gt;
&lt;p&gt;A discussion around these differences and how they relate to R can be found &lt;a href="http://r.789695.n4.nabble.com/quot-make-check-quot-fails-on-lapack-R-and-stats-Ex-R-td4698672.html"&gt;on nabble&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How fast is it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So is it worth it? I ran a benchmark called &lt;a href="https://github.com/mikecroucher/HPC_Examples/blob/35de11e7c47bc278b15a64fb77c5575b074e1a47/languages/R/linear_algebra/linear_algebra_bench.r#L19"&gt;linear_algebra_bench.r&lt;/a&gt; that implemented 5 tests&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MatMul - Multiplies two random 1000 x 5000 matrices together&lt;/li&gt;
&lt;li&gt;Chol - Cholesky decomposition of a 5000 x 5000 random matrix&lt;/li&gt;
&lt;li&gt;SVD - Singular Value Decompisition of a 10000 x 2000 random matrix&lt;/li&gt;
&lt;li&gt;PCA - Principle component analysis of a 10000 x 2000 random matrix&lt;/li&gt;
&lt;li&gt;LDA - A Linear Discriminant Analysis problem&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Run time of these operations compared to Iceberg's standard install of R is shown in the table below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Iceberg submission scripts for these can be found in the &lt;a href="https://github.com/mikecroucher/HPC_Examples"&gt;HPC Examples repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Execution time in seconds (Mean of 5 independent runs) &lt;/strong&gt;&lt;/p&gt;
&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-yw4l{vertical-align:top}
&lt;/style&gt;

&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-yw4l"&gt;&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;MatMul&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;Chol&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;SVD&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;PCA&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;LDA&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Standard R&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;134.70&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;20.95&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;46.56&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;179.60&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;132.40&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with sequential MKL&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;12.19&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;2.24&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;9.13&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;24.58&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;31.32&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with parallel MKL (2 cores)&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;7.21&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1.60&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;5.43&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;14.66&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;23.54&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with parallel MKL (4 cores)&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;3.24&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1.17&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;3.34&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;7.87&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;20.63&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with parallel MKL (8 cores)&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1.71&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;0.38&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1.99&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;5.33&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;15.82&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with parallel MKL (16 cores)&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;0.96&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;0.28&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1.60&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;4.05&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;13.65&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img alt="" src="http://rse.shef.ac.uk/images/matmul_r_intel.png"&gt;&lt;/p&gt;
&lt;p&gt;Another way of viewing these results is to see the speed up compared to the standard install of R. &lt;strong&gt;Even on a single CPU core, the Intel builds are between 4 and 11 times faster than the standard builds&lt;/strong&gt;.  Making use of 16 cores takes this up to &lt;strong&gt;141 times faster in the case of Matrix-Matrix Multiplication&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speed up compared to standard R&lt;/strong&gt;&lt;/p&gt;
&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-yw4l{vertical-align:top}
&lt;/style&gt;

&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-yw4l"&gt; &lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;MatMul&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;Chol&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;SVD&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;PCA&lt;/th&gt;
    &lt;th class="tg-yw4l"&gt;LDA&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Standard R&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with sequential MKL&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;11&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;9&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;5&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;7&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;4&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with parallel MKL (2 cores)&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;19&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;13&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;9&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;12&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;6&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with parallel MKL (4 cores)&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;42&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;18&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;14&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;23&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;6&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with parallel MKL (8 cores)&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;79&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;55&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;23&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;34&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;8&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-yw4l"&gt;Intel R with parallel MKL (16 cores)&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;141&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;75&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;29&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;44&lt;/td&gt;
    &lt;td class="tg-yw4l"&gt;10&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Parallel environment&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The type of parallelisation in use here is &lt;a href="http://openmp.org/wp/"&gt;OpenMP&lt;/a&gt;. As such, you need to use Iceberg's openmp environment.  That is, if you want 8 cores (say), add the following to your submission  script&lt;/p&gt;
&lt;pre&gt;
#$ -pe openmp 8
export OMP_NUM_THREADS=8
&lt;/pre&gt;

&lt;p&gt;Using OpenMP limits the number of cores you can use per job to the number available on a single node. At the time of writing, this is 16.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How many cores: Finding the sweet spot&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Note that everything is fastest when using 16 cores! As such, it may be tempting to always use 16 cores for your jobs. This will almost always be a mistake.
It may be that the aspect of your code that's accelerated by this build doesn't account for much of the runtime of your problem. As such, those 16 cores will sit idle most of the time -- wasting resources.  &lt;/p&gt;
&lt;p&gt;You'll also spend a lot longer waiting in the queue for 16 cores than you will for 2 cores which may swap any speed gains.&lt;/p&gt;
&lt;p&gt;You should always perform scaling experiments before deciding how many cores to use for your jobs. Consider the Linear Discriminant Analysis problem, for example. Using just one core, Intel build gives us a 4 times speed-up compared to the standard build. Moving to 8 cores only makes it twice as fast again. As such, if you had lots of these jobs to do, your throughput would be higher running lots of single core jobs compared to lots of 8 core jobs.&lt;/p&gt;
&lt;p&gt;If matrix-matrix multiply dominates your runtime, on the other hand, it may well be worth using 16 cores.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using this version of R for your own work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a user, there are a few things you need to be aware of with the Intel builds of R so I've created a separate documentation page for them.  This is currently at
&lt;a href="http://docs.hpc.shef.ac.uk/en/latest/iceberg/software/apps/intel_r.html"&gt;http://docs.hpc.shef.ac.uk/en/latest/iceberg/software/apps/intel_r.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My recommendation for using these builds is to work through the following procedure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensure that your code runs with Iceberg's standard version of R and produce a test result.&lt;/li&gt;
&lt;li&gt;In the first instance, switch to the sequential version of the Intel R build. In the best case, this will just require changing the module. You may also need to install some of your packages since the Intel build has a separate packages directory to the standard build.&lt;/li&gt;
&lt;li&gt;If you see speed-up &lt;strong&gt;and&lt;/strong&gt; the results are consistent with your test result, try the parallel version. Initially start with 2 cores and move upwards to find the sweet spot.&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/intel-R-iceberg/</guid><pubDate>Mon, 12 Sep 2016 00:31:35 GMT</pubDate></item><item><title>The University of Sheffield named an NVIDIA GPU Education Center</title><link>http://rse.shef.ac.uk/blog/the-university-of-sheffield-named-an-nvidia-gpu-education-center/</link><dc:creator>Paul Richmond</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img src="http://rse.shef.ac.uk/images/NVIDIA_education.jpg" alt="Sheffield NVIDIA Education Centre" align="right" width="20%"&gt;&lt;/p&gt;
&lt;p&gt;This week I am very pleased to announce that the University of Sheffield has been awarded the status of an NVIDIA CUDA Education Centre.&lt;/p&gt;
&lt;p&gt;The faculty of Engineering has featured this in its latest &lt;a href="http://www.sheffield.ac.uk/faculty/engineering/news/nvidia-1.587003"&gt;faculty newsletter&lt;/a&gt; and the Department of Computer Science has published more details in a &lt;a href="http://www.sheffield.ac.uk/dcs/latest-news/nvidia-1.587214"&gt;news feature&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;But what does this mean to the RSE community at Sheffield and beyond?&lt;/h3&gt;
&lt;p&gt;The recognition of being an NVIDIA education centre is a reflection of the teaching that is provided by The University of Sheffield on the subject of GPU computing. In case you are unaware of what teaching there is, I have a 4th year and Masters teaching module &lt;a href="http://paulrichmond.shef.ac.uk/teaching/COM4521/"&gt;COM4521/COM6521&lt;/a&gt; which ran for the first time in the 2015/2016 Spring Semester. This course will be run annually and is open to research staff as well as taught students. Last time there was roughly a 50:50 mix including senior research staff and PhD students. It is much more involved that the one or two day courses which typically give only an introduction to GPU programming. If you are a researcher looking to exploit GPU performance in your research then this course is an opportunity to learn some new skills.&lt;/p&gt;
&lt;p&gt;In the future this course will be made freely available so even researchers outside of The University of Sheffield will be able to go through the notes and worked examples (lab sheets). &lt;/p&gt;
&lt;p&gt;Some of the other benefits of being an NVIDIA Eduction (and also an NVIDIA Research) centre are;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Access to NVIDIA GPU hardware and software (via Iceberg and in the Diamond labs) &lt;/li&gt;
&lt;li&gt;Significant discount on Tesla hardware purchases&lt;/li&gt;
&lt;li&gt;Access to NVIDIA parallel programming experts and resources&lt;/li&gt;
&lt;li&gt;Access to educational webinars and an array of teaching materials&lt;/li&gt;
&lt;li&gt;Free in the cloud GPU programming training at &lt;a href="https://nvidia.qwiklab.com/"&gt;nvidia.qwiklab.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support in the form of letters of support (with contributions in kind) for research proposals with emphasis on GPU computing or deep learning&lt;/li&gt;
&lt;li&gt;Joint promotion, public relations, and press activities with NVIDIA&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other Training Opportunities&lt;/h3&gt;
&lt;p&gt;Through &lt;a href="http://rse.shef.ac.uk/"&gt;RSE Sheffield&lt;/a&gt; and &lt;a href="http://gpucomputing.sites.sheffield.ac.uk"&gt;GPUComputing@Sheffield&lt;/a&gt; shorter courses for GPU computing are also available. I will be announcing dates for 1-2 day CUDA courses shortly and am working with CICS in developing new Python CUDA material. &lt;/p&gt;
&lt;p&gt;For those that missed the sign-up, we are also running a two day deep learning with GPUs course in July. The places for this were in high demand and filled up within a day. This course will be repeated in due time and material from the course will be made available off-line.&lt;/p&gt;
&lt;p&gt;Other GPU announcements will be made on both this RSE blog and on the &lt;a href="https://groups.google.com/a/sheffield.ac.uk/forum/#!forum/gpucomputing"&gt;GPUComputing@Sheffield mailing list&lt;/a&gt;. Expect some exciting new hardware and software once the Iceberg upgrade is complete (shortly).&lt;/p&gt;
&lt;p&gt;Paul&lt;/p&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/the-university-of-sheffield-named-an-nvidia-gpu-education-center/</guid><pubDate>Sat, 02 Jul 2016 16:50:04 GMT</pubDate></item><item><title>NAG Fortran Compiler 6.1 released</title><link>http://rse.shef.ac.uk/blog/NAG_6_1/</link><dc:creator>Mike Croucher</dc:creator><description>&lt;div&gt;&lt;p&gt;All members of the University are entitled to download and use the NAG Fortran Compiler under the terms of our site license.&lt;/p&gt;
&lt;p&gt;Version 6.1 has just been released:&lt;/p&gt;
&lt;p&gt;&lt;img alt="NAG Fortran Compiler Screenshot" src="http://rse.shef.ac.uk/images/macFB2a.PNG"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Details at &lt;a href="http://www.nag.co.uk/nag-compiler"&gt;http://www.nag.co.uk/nag-compiler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download at &lt;a href="http://www.nag.co.uk/content/downloads-nag-fortran-compiler-versions"&gt;http://www.nag.co.uk/content/downloads-nag-fortran-compiler-versions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Obtain licenses from &lt;strong&gt;support@nag.co.uk&lt;/strong&gt; - Ensure that you email them from your @Sheffield.ac.uk email address&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><guid>http://rse.shef.ac.uk/blog/NAG_6_1/</guid><pubDate>Thu, 16 Jun 2016 13:50:28 GMT</pubDate></item></channel></rss>